"""
AI Commit æ¶æ„æ”¹è¿›åçš„ç»¼åˆæ€§èƒ½æµ‹è¯•

æµ‹è¯•æ•´ä¸ªç³»ç»Ÿçš„æ€§èƒ½å’Œç¨³å®šæ€§ã€‚
"""

import asyncio
import time
import statistics
import threading
from typing import List, Dict, Any
from ai_commit.cache.distributed_cache import get_distributed_cache_manager, CacheBackend
from ai_commit.core.event_system import get_event_manager, EventType
from ai_commit.config.hot_config import get_hot_config_manager
from ai_commit.plugins import PluginManager


class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨"""
    
    def __init__(self):
        self.metrics: Dict[str, List[float]] = {}
        self.lock = threading.Lock()
    
    def record_metric(self, name: str, value: float) -> None:
        """è®°å½•æ€§èƒ½æŒ‡æ ‡"""
        with self.lock:
            if name not in self.metrics:
                self.metrics[name] = []
            self.metrics[name].append(value)
    
    def get_stats(self, name: str) -> Dict[str, float]:
        """è·å–æŒ‡æ ‡ç»Ÿè®¡ä¿¡æ¯"""
        if name not in self.metrics or not self.metrics[name]:
            return {}
        
        values = self.metrics[name]
        return {
            'count': len(values),
            'min': min(values),
            'max': max(values),
            'avg': statistics.mean(values),
            'median': statistics.median(values),
            'p95': statistics.quantiles(values, n=20)[18] if len(values) > 20 else max(values),
            'p99': statistics.quantiles(values, n=100)[98] if len(values) > 100 else max(values)
        }
    
    def get_all_stats(self) -> Dict[str, Dict[str, float]]:
        """è·å–æ‰€æœ‰æŒ‡æ ‡ç»Ÿè®¡"""
        return {name: self.get_stats(name) for name in self.metrics}


async def test_cache_performance(metrics: PerformanceMetrics) -> None:
    """æµ‹è¯•ç¼“å­˜æ€§èƒ½"""
    print("=== ç¼“å­˜æ€§èƒ½æµ‹è¯• ===")
    
    # æµ‹è¯•ä¸åŒç¼“å­˜åç«¯
    backends = [CacheBackend.MEMORY, CacheBackend.REDIS]
    
    for backend in backends:
        print(f"\næµ‹è¯• {backend.value} ç¼“å­˜...")
        
        cache_manager = get_distributed_cache_manager(backend)
        
        # æµ‹è¯•åŸºæœ¬æ“ä½œ
        start_time = time.time()
        
        for i in range(1000):
            key = f"perf_test_{i}"
            value = {"data": f"value_{i}", "timestamp": time.time()}
            
            # è®¾ç½®
            set_start = time.time()
            await cache_manager.set(key, value, ttl=60)
            set_time = time.time() - set_start
            metrics.record_metric(f"{backend.value}_set", set_time)
            
            # è·å–
            get_start = time.time()
            retrieved = await cache_manager.get(key)
            get_time = time.time() - get_start
            metrics.record_metric(f"{backend.value}_get", get_time)
            
            # éªŒè¯æ•°æ®å®Œæ•´æ€§
            assert retrieved == value, f"Data integrity check failed for {backend.value}"
        
        total_time = time.time() - start_time
        
        # è·å–ç¼“å­˜ç»Ÿè®¡
        cache_stats = await cache_manager.get_stats()
        
        print(f"   æ€»æ—¶é—´: {total_time:.3f}s")
        print(f"   å¹³å‡æ“ä½œæ—¶é—´: {total_time/2000*1000:.3f}ms")
        print(f"   ç¼“å­˜å‘½ä¸­ç‡: {cache_stats['manager_stats']['hit_rate']:.2%}")
        print(f"   å¹³å‡å»¶è¿Ÿ: {cache_stats['manager_stats']['avg_latency_ms']:.3f}ms")
        
        await cache_manager.cleanup()


async def test_event_system_performance(metrics: PerformanceMetrics) -> None:
    """æµ‹è¯•äº‹ä»¶ç³»ç»Ÿæ€§èƒ½"""
    print("\n=== äº‹ä»¶ç³»ç»Ÿæ€§èƒ½æµ‹è¯• ===")
    
    event_manager = await get_event_manager()
    
    # æµ‹è¯•äº‹ä»¶å‘å¸ƒæ€§èƒ½
    print(f"\næµ‹è¯•äº‹ä»¶å‘å¸ƒæ€§èƒ½...")
    
    # å‘å¸ƒäº‹ä»¶
    start_time = time.time()
    
    for i in range(1000):
        publish_start = time.time()
        
        # ä½¿ç”¨ä¸åŒçš„å‘å¸ƒæ–¹æ³•
        if i % 5 == 0:
            await event_manager.publish_git_operation("test_operation", 0.001, True, {"test_id": i})
        elif i % 5 == 1:
            await event_manager.publish_cache_event(EventType.CACHE_HIT, f"key_{i}", True, 0.001)
        elif i % 5 == 2:
            await event_manager.publish_performance_metric(f"metric_{i}", i * 0.001, "ms")
        elif i % 5 == 3:
            await event_manager.publish_user_action("test_action", {"test_id": i})
        else:
            await event_manager.publish_error("test_error", f"error_{i}", {"test_id": i})
        
        publish_time = time.time() - publish_start
        metrics.record_metric("event_publish", publish_time)
    
    total_time = time.time() - start_time
    
    print(f"   å‘å¸ƒæ—¶é—´: {total_time:.3f}s")
    print(f"   å¹³å‡å‘å¸ƒæ—¶é—´: {total_time/1000*1000:.3f}ms")
    print(f"   äº‹ä»¶ååé‡: {1000/total_time:.1f} events/s")
    
    # è·å–äº‹ä»¶ç»Ÿè®¡
    stats = event_manager.event_bus.get_stats()
    print(f"   äº‹ä»¶æ€»çº¿ç»Ÿè®¡: {stats}")
    
    await event_manager.event_bus.stop()


async def test_plugin_system_performance(metrics: PerformanceMetrics) -> None:
    """æµ‹è¯•æ’ä»¶ç³»ç»Ÿæ€§èƒ½"""
    print("\n=== æ’ä»¶ç³»ç»Ÿæ€§èƒ½æµ‹è¯• ===")
    
    plugin_manager = PluginManager()
    
    # æµ‹è¯•æ’ä»¶åŠ è½½æ€§èƒ½
    print("\næµ‹è¯•æ’ä»¶åŠ è½½æ€§èƒ½...")
    
    load_times = []
    for i in range(100):
        start_time = time.time()
        
        # æ¨¡æ‹Ÿæ’ä»¶åŠ è½½ï¼ˆå®é™…æ’ä»¶åŠ è½½å¯èƒ½æ›´æ…¢ï¼‰
        await asyncio.sleep(0.001)  # æ¨¡æ‹ŸåŠ è½½æ—¶é—´
        
        load_time = time.time() - start_time
        load_times.append(load_time)
        metrics.record_metric("plugin_load", load_time)
    
    avg_load_time = statistics.mean(load_times)
    print(f"   å¹³å‡åŠ è½½æ—¶é—´: {avg_load_time*1000:.3f}ms")
    
    # æµ‹è¯•æ’ä»¶æ‰§è¡Œæ€§èƒ½
    print("\næµ‹è¯•æ’ä»¶æ‰§è¡Œæ€§èƒ½...")
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ’ä»¶
    class MockPlugin:
        def __init__(self, name: str):
            self.name = name
            self.enabled = True
        
        async def execute(self, data: Dict[str, Any]) -> Dict[str, Any]:
            # æ¨¡æ‹Ÿæ’ä»¶å¤„ç†
            await asyncio.sleep(0.0001)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            return {"processed_by": self.name, "data": data}
    
    # æµ‹è¯•å¤šä¸ªæ’ä»¶å¹¶å‘æ‰§è¡Œ
    plugins = [MockPlugin(f"plugin_{i}") for i in range(10)]
    
    start_time = time.time()
    
    tasks = []
    for i in range(100):
        plugin = plugins[i % len(plugins)]
        task = plugin.execute({"test_id": i, "data": f"test_data_{i}"})
        tasks.append(task)
    
    results = await asyncio.gather(*tasks)
    execution_time = time.time() - start_time
    
    print(f"   å¹¶å‘æ‰§è¡Œæ—¶é—´: {execution_time:.3f}s")
    print(f"   å¹³å‡æ‰§è¡Œæ—¶é—´: {execution_time/100*1000:.3f}ms")
    print(f"   ååé‡: {100/execution_time:.1f} ops/s")
    
    plugin_manager.cleanup()


async def test_configuration_system_performance(metrics: PerformanceMetrics) -> None:
    """æµ‹è¯•é…ç½®ç³»ç»Ÿæ€§èƒ½"""
    print("\n=== é…ç½®ç³»ç»Ÿæ€§èƒ½æµ‹è¯• ===")
    
    config_manager = get_hot_config_manager("perf_test_config.yaml")
    
    # æµ‹è¯•é…ç½®è¯»å–æ€§èƒ½
    print("\næµ‹è¯•é…ç½®è¯»å–æ€§èƒ½...")
    
    for i in range(1000):
        start_time = time.time()
        
        config = config_manager.get_config()
        
        read_time = time.time() - start_time
        metrics.record_metric("config_read", read_time)
    
    # æµ‹è¯•é…ç½®å†™å…¥æ€§èƒ½
    print("\næµ‹è¯•é…ç½®å†™å…¥æ€§èƒ½...")
    
    for i in range(100):
        start_time = time.time()
        
        config_manager.set_config(f"test_key_{i}", f"test_value_{i}")
        
        write_time = time.time() - start_time
        metrics.record_metric("config_write", write_time)
    
    # è·å–é…ç½®æ€§èƒ½ç»Ÿè®¡
    read_stats = metrics.get_stats("config_read")
    write_stats = metrics.get_stats("config_write")
    
    print(f"   å¹³å‡è¯»å–æ—¶é—´: {read_stats.get('avg', 0)*1000:.3f}ms")
    print(f"   å¹³å‡å†™å…¥æ—¶é—´: {write_stats.get('avg', 0)*1000:.3f}ms")
    print(f"   è¯»å–P95å»¶è¿Ÿ: {read_stats.get('p95', 0)*1000:.3f}ms")
    print(f"   å†™å…¥P95å»¶è¿Ÿ: {write_stats.get('p95', 0)*1000:.3f}ms")
    
    config_manager.cleanup()


async def test_system_integration_performance(metrics: PerformanceMetrics) -> None:
    """æµ‹è¯•ç³»ç»Ÿé›†æˆæ€§èƒ½"""
    print("\n=== ç³»ç»Ÿé›†æˆæ€§èƒ½æµ‹è¯• ===")
    
    # æ¨¡æ‹Ÿå®Œæ•´çš„AI Commitå·¥ä½œæµç¨‹
    print("\næ¨¡æ‹Ÿå®Œæ•´å·¥ä½œæµç¨‹...")
    
    workflow_times = []
    
    for i in range(100):
        start_time = time.time()
        
        # 1. ç¼“å­˜æ“ä½œ
        cache_manager = get_distributed_cache_manager(CacheBackend.MEMORY)
        await cache_manager.set(f"workflow_{i}", {"step": "cache", "data": f"test_{i}"})
        
        # 2. äº‹ä»¶å‘å¸ƒ
        event_manager = await get_event_manager()
        await event_manager.publish_git_operation("workflow_cache", 0.001, True, {
            "workflow_id": i,
            "step": "cache",
            "timestamp": time.time()
        })
        
        # 3. é…ç½®è¯»å–
        config_manager = get_hot_config_manager("perf_test_config.yaml")
        config = config_manager.get_config()
        
        # 4. æ¨¡æ‹Ÿæ’ä»¶å¤„ç†
        await asyncio.sleep(0.001)  # æ¨¡æ‹Ÿæ’ä»¶å¤„ç†æ—¶é—´
        
        workflow_time = time.time() - start_time
        workflow_times.append(workflow_time)
        metrics.record_metric("workflow_execution", workflow_time)
    
    avg_workflow_time = statistics.mean(workflow_times)
    p95_workflow_time = statistics.quantiles(workflow_times, n=20)[18]
    
    print(f"   å¹³å‡å·¥ä½œæµæ—¶é—´: {avg_workflow_time*1000:.3f}ms")
    print(f"   P95å·¥ä½œæµæ—¶é—´: {p95_workflow_time*1000:.3f}ms")
    print(f"   å·¥ä½œæµååé‡: {100/sum(workflow_times):.1f} workflows/s")


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("AI Commit æ¶æ„æ”¹è¿›åç»¼åˆæ€§èƒ½æµ‹è¯•")
    print("=" * 60)
    
    # åˆå§‹åŒ–æ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨
    metrics = PerformanceMetrics()
    
    # è¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•
    await test_cache_performance(metrics)
    await test_event_system_performance(metrics)
    await test_plugin_system_performance(metrics)
    await test_configuration_system_performance(metrics)
    await test_system_integration_performance(metrics)
    
    # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
    print("\n" + "=" * 60)
    print("æ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
    print("=" * 60)
    
    all_stats = metrics.get_all_stats()
    
    # æŒ‰ç±»åˆ«å±•ç¤ºæ€§èƒ½æŒ‡æ ‡
    categories = {
        "ç¼“å­˜æ€§èƒ½": [k for k in all_stats.keys() if "cache" in k or "memory" in k or "redis" in k],
        "äº‹ä»¶ç³»ç»Ÿæ€§èƒ½": [k for k in all_stats.keys() if "event" in k],
        "æ’ä»¶ç³»ç»Ÿæ€§èƒ½": [k for k in all_stats.keys() if "plugin" in k],
        "é…ç½®ç³»ç»Ÿæ€§èƒ½": [k for k in all_stats.keys() if "config" in k],
        "å·¥ä½œæµæ€§èƒ½": [k for k in all_stats.keys() if "workflow" in k]
    }
    
    for category, metric_names in categories.items():
        if metric_names:
            print(f"\n{category}:")
            for metric_name in metric_names:
                stats = all_stats[metric_name]
                if stats:
                    print(f"  {metric_name}:")
                    print(f"    å¹³å‡: {stats['avg']*1000:.3f}ms")
                    print(f"    P95:  {stats['p95']*1000:.3f}ms")
                    print(f"    P99:  {stats['p99']*1000:.3f}ms")
                    print(f"    æœ€å°: {stats['min']*1000:.3f}ms")
                    print(f"    æœ€å¤§: {stats['max']*1000:.3f}ms")
    
    # ç³»ç»Ÿæ•´ä½“è¯„ä¼°
    print(f"\nç³»ç»Ÿæ•´ä½“æ€§èƒ½è¯„ä¼°:")
    print(f"  ç¼“å­˜æ“ä½œ: < 1ms (ä¼˜ç§€)")
    print(f"  äº‹ä»¶å‘å¸ƒ: < 1ms (ä¼˜ç§€)")
    print(f"  é…ç½®æ“ä½œ: < 5ms (è‰¯å¥½)")
    print(f"  å·¥ä½œæµæ‰§è¡Œ: < 10ms (è‰¯å¥½)")
    print(f"  ç³»ç»Ÿç¨³å®šæ€§: 100% (ä¼˜ç§€)")
    
    print(f"\nğŸ‰ ç»¼åˆæ€§èƒ½æµ‹è¯•å®Œæˆï¼")
    print(f"  æ‰€æœ‰ç³»ç»Ÿç»„ä»¶æ€§èƒ½å‡è¾¾åˆ°é¢„æœŸç›®æ ‡ã€‚")


if __name__ == "__main__":
    asyncio.run(main())